---
phase: 02-code-generation
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/config.py
  - backend/app/llm.py
autonomous: true
requirements:
  - CODE-01
must_haves:
  truths:
    - call_llm_vision accepts base64 image + text, returns plain text
    - Code generation uses Gemini 2.5 Pro (vision-capable)
    - Code fence stripping extracts JSX from markdown output
  artifacts:
    - path: backend/app/config.py
      provides: CODE_GEN_MODEL config
      contains: "gemini-2.5-pro|CODE_GEN"
    - path: backend/app/llm.py
      provides: call_llm_vision
      contains: "call_llm_vision"
  key_links:
    - from: backend/app/llm.py
      to: litellm
      via: completion with multimodal content
      pattern: "litellm|completion"
    - from: backend/app/llm.py
      to: config.py
      via: CODE_GEN_MODEL
      pattern: "CODE_GEN"

---

<objective>
Add LLM vision support for multimodal generation. Code gen needs Figma thumbnail (base64) + structured context. Gemini 2.5 Pro supports vision; no response_format for code output.

Purpose: Design-to-code requires visual input. LLM must see the design image. Use Gemini 2.5 Pro per CONTEXT.md.
Output: call_llm_vision(), CODE_GEN_MODEL config, code fence stripping.
</objective>

<execution_context>
@/Users/shubham/.claude/get-shit-done/workflows/execute-plan.md
@/Users/shubham/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-code-generation/02-CONTEXT.md
@.planning/phases/02-code-generation/02-RESEARCH.md
@backend/app/llm.py
@backend/app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Code gen model config</name>
  <files>backend/app/config.py</files>
  <action>
    Add to config.py (separate from research fallback_chain):
    CODE_GEN_MODEL = "gemini/gemini-2.5-pro" or "gemini/gemini-2.5-pro-preview" — verify exact litellm model string in docs.
    Code generation uses this model only (no fallback chain for Phase 2 — per CONTEXT.md V1 model).
    Do NOT add response_format for code gen — 02-RESEARCH.md says Gemini JSON mode conflicts with vision. Code output is plain text.
  </action>
  <verify>CODE_GEN_MODEL is importable. Model string valid for litellm.</verify>
  <done>Code gen model config in place.</done>
</task>

<task type="auto">
  <name>Task 2: call_llm_vision implementation</name>
  <files>backend/app/llm.py</files>
  <action>
    Add async def call_llm_vision(messages: list[dict], image_base64: str | None) -> str.
    If image_base64: prepend to first user message content as list: [{"type": "image_url", "image_url": {"url": f"data:image/png;base64,{image_base64}"}}, {"type": "text", "text": "..."}]. If image is None, use messages as-is.
    Call litellm.completion with model=CODE_GEN_MODEL, messages=messages, temperature=0.3, max_tokens=8000. Do NOT pass response_format or response_format={"type": "json_object"}.
    Return response.choices[0].message.content as str.
    Log per AGENTS.md: llm call started/succeeded/failed for vision call.
    Fetch thumbnail URL to base64: caller (codegen endpoint) will do this — llm.py does not fetch. call_llm_vision receives image_base64 already encoded.
  </action>
  <verify>call_llm_vision with text-only messages returns content. With image_base64, litellm accepts multimodal content. No response_format crash.</verify>
  <done>call_llm_vision works for multimodal input.</done>
</task>

<task type="auto">
  <name>Task 3: Code fence stripping</name>
  <files>backend/app/llm.py</files>
  <action>
    Add def strip_code_fences(text: str) -> str:
    If text contains ```jsx or ```tsx or ```javascript, extract content between fences. Regex: r'```(?:jsx|tsx|javascript)?\s*\n(.*?)```' (dotall). Return first match or original text if no match.
    Used when LLM returns markdown-wrapped code. Export for use in codegen endpoint.
  </action>
  <verify>strip_code_fences('```jsx\nconst x = 1;\n```') returns 'const x = 1;\n'. strip_code_fences('no fences') returns 'no fences'.</verify>
  <done>Code fence stripping extracts JSX from markdown.</done>
</task>

</tasks>

<verification>
- CODE_GEN_MODEL configured
- call_llm_vision accepts base64 image + text
- No response_format for code gen (avoids Gemini conflict)
- strip_code_fences extracts JSX
</verification>

<success_criteria>
- Vision LLM path ready for code generation
- Code output extracted from markdown fences
</success_criteria>

<output>
After completion, create `.planning/phases/02-code-generation/02-03-SUMMARY.md`
</output>
