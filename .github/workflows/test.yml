name: Tests

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  schedule:
    # Run nightly at 2 AM UTC for full prompt eval suite
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      run_evals:
        description: 'Run prompt evaluation tests'
        required: false
        default: 'false'
        type: boolean

env:
  PYTHON_VERSION: '3.12'

jobs:
  # ─────────────────────────────────────────────────────────────────────────────
  # Unit Tests (Every Commit - Mocked, No API Keys Required)
  # ─────────────────────────────────────────────────────────────────────────────
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: backend

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: backend/requirements.txt

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run unit tests
        run: |
          pytest tests -v \
            --ignore=tests/evals \
            --tb=short \
            --color=yes \
            -x  # Stop on first failure for faster feedback
        env:
          # Test environment variables (mocked, not real)
          GEMINI_API_KEY: test-gemini-key
          OPENAI_API_KEY: test-openai-key
          ANTHROPIC_API_KEY: test-anthropic-key
          TAVILY_API_KEY: test-tavily-key
          SERPER_API_KEY: test-serper-key
          JINA_API_KEY: test-jina-key
          SUPABASE_URL: https://test.supabase.co
          SUPABASE_SERVICE_KEY: test-supabase-key
          ENVIRONMENT: test

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: unit-test-results
          path: backend/.pytest_cache
          retention-days: 7

  # ─────────────────────────────────────────────────────────────────────────────
  # Prompt Evaluation Tests (On prompts.py Changes, Nightly, or Manual Trigger)
  # ─────────────────────────────────────────────────────────────────────────────
  prompt-evals:
    name: Prompt Evaluation
    runs-on: ubuntu-latest
    # Run when:
    # 1. prompts.py is modified
    # 2. Scheduled nightly run
    # 3. Manual trigger with run_evals=true
    if: |
      github.event_name == 'schedule' ||
      (github.event_name == 'workflow_dispatch' && inputs.run_evals == 'true') ||
      (github.event_name == 'push' && contains(github.event.head_commit.modified, 'prompts.py')) ||
      (github.event_name == 'pull_request' && contains(github.event.pull_request.changed_files, 'prompts.py'))
    defaults:
      run:
        working-directory: backend

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 2  # Need to check changed files

      - name: Check if prompts.py changed (PR)
        if: github.event_name == 'pull_request'
        id: check-prompts
        run: |
          if git diff --name-only HEAD~1 | grep -q "prompts.py"; then
            echo "changed=true" >> $GITHUB_OUTPUT
          else
            echo "changed=false" >> $GITHUB_OUTPUT
          fi

      - name: Skip if prompts.py not changed (PR only)
        if: |
          github.event_name == 'pull_request' && 
          steps.check-prompts.outputs.changed != 'true'
        run: |
          echo "prompts.py not changed, skipping eval tests"
          exit 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: backend/requirements.txt

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run prompt evaluation tests
        run: |
          pytest tests/evals -v \
            -m eval \
            --tb=short \
            --color=yes \
            --timeout=300  # 5 min timeout per test
        env:
          # Real API keys from secrets
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          TAVILY_API_KEY: ${{ secrets.TAVILY_API_KEY }}
          SERPER_API_KEY: ${{ secrets.SERPER_API_KEY }}
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          ENVIRONMENT: test

      - name: Upload eval results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: prompt-eval-results
          path: backend/.pytest_cache
          retention-days: 30

  # ─────────────────────────────────────────────────────────────────────────────
  # Lint Check (Every Commit)
  # ─────────────────────────────────────────────────────────────────────────────
  lint:
    name: Lint
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: backend

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install linting tools
        run: |
          python -m pip install --upgrade pip
          pip install ruff

      - name: Run ruff check
        run: ruff check . --output-format=github
        continue-on-error: true  # Don't fail the build on lint errors for now

      - name: Run ruff format check
        run: ruff format --check .
        continue-on-error: true

  # ─────────────────────────────────────────────────────────────────────────────
  # Type Check (Every Commit)
  # ─────────────────────────────────────────────────────────────────────────────
  type-check:
    name: Type Check
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: backend

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          cache-dependency-path: backend/requirements.txt

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pyright

      - name: Run type check
        run: pyright app
        continue-on-error: true  # Don't fail the build on type errors for now

  # ─────────────────────────────────────────────────────────────────────────────
  # Summary Job
  # ─────────────────────────────────────────────────────────────────────────────
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, lint, type-check]
    if: always()

    steps:
      - name: Check test results
        run: |
          if [ "${{ needs.unit-tests.result }}" == "failure" ]; then
            echo "❌ Unit tests failed"
            exit 1
          fi
          echo "✅ All required checks passed"

      - name: Report status
        run: |
          echo "## Test Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Job | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|-----|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Unit Tests | ${{ needs.unit-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Lint | ${{ needs.lint.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Type Check | ${{ needs.type-check.result }} |" >> $GITHUB_STEP_SUMMARY
