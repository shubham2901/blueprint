---
phase: 02-code-generation
plan: 06
type: execute
wave: 3
depends_on:
  - 02-04
files_modified:
  - backend/tests/conftest.py
  - backend/tests/test_figma_context.py
  - backend/tests/test_codegen.py
  - backend/tests/test_codegen_api.py
autonomous: true
requirements:
  - CODE-01
must_haves:
  truths:
    - Unit tests exist for design context transformer (pure function, no mocks needed)
    - Unit tests exist for call_llm_vision (mocked litellm)
    - Unit tests exist for strip_code_fences with JSX/TSX content
    - Integration tests exist for POST /api/code/generate (mocked LLM + Figma + DB)
    - Integration tests exist for GET /api/code/session (mocked DB)
    - All tests pass with `pytest backend/tests/`
    - All external services mocked — no real API calls
  artifacts:
    - path: backend/tests/test_figma_context.py
      provides: Unit tests for design context transformer
      contains: "transform_design_context|test_"
    - path: backend/tests/test_codegen.py
      provides: Unit tests for LLM vision and code fence stripping
      contains: "call_llm_vision|strip_code_fences|test_"
    - path: backend/tests/test_codegen_api.py
      provides: Integration tests for code generation API endpoints
      contains: "api/code/generate|api/code/session|test_"
  key_links:
    - from: backend/tests/test_figma_context.py
      to: backend/app/figma_context.py
      via: import transform_design_context
      pattern: "transform_design_context"
    - from: backend/tests/test_codegen_api.py
      to: backend/app/api/codegen.py
      via: HTTP client test calls
      pattern: "api/code"

---

<objective>
Tests for Phase 2 code generation: unit tests for pure functions (transformer, code fence stripping), unit tests for LLM vision (mocked), and integration tests for API endpoints (mocked LLM + Figma + DB). Follows existing test conventions from backend/tests/.

Purpose: Verify Phase 2 components work correctly in isolation and together. Catch regressions. All external services mocked per AGENTS.md testing conventions.
Output: test_figma_context.py, test_codegen.py, test_codegen_api.py, conftest.py updates.
</objective>

<execution_context>
@/Users/shubham/.claude/get-shit-done/workflows/execute-plan.md
@/Users/shubham/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-code-generation/02-CONTEXT.md
@backend/tests/conftest.py
@backend/tests/test_api.py
@backend/tests/test_llm.py
@backend/app/figma_context.py
@backend/app/llm.py
@backend/app/api/codegen.py
@backend/app/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Unit tests for design context transformer</name>
  <files>backend/tests/test_figma_context.py</files>
  <action>
    Create backend/tests/test_figma_context.py. The transformer is a pure function — no mocks needed for most tests.
    Test class: TestTransformDesignContext

    **Tests to write:**
    1. test_transform_empty_input — empty dict returns valid structure with empty tree
    2. test_transform_minimal_frame — single FRAME node produces correct frame metadata (name, width, height)
    3. test_transform_text_node — TEXT node extracts characters, fontFamily, fontSize, fontWeight, color
    4. test_transform_rectangle_node — RECTANGLE extracts fills (solid color), cornerRadius, dimensions
    5. test_transform_layout_properties — FRAME with layoutMode=VERTICAL extracts mode, gap (itemSpacing), padding
    6. test_transform_nested_children — parent with children produces nested tree structure
    7. test_transform_depth_limit — deeply nested tree (>5 levels) is pruned at max_depth
    8. test_transform_vector_marked_as_icon — VECTOR node gets icon=True flag
    9. test_transform_boolean_operation_marked_as_icon — BOOLEAN_OPERATION node gets icon=True flag
    10. test_transform_image_fill_detected — node with fills containing type IMAGE gets image=True and dimensions
    11. test_transform_components_passed_through — raw components dict passed through to output
    12. test_transform_styles_passed_through — raw styles dict passed through to output
    13. test_transform_malformed_node — node missing document key handled gracefully (no crash)
    14. test_transform_output_is_json_serializable — output can be json.dumps'd without error

    **Sample input (reuse across tests):**
    Build a sample_figma_response fixture that matches the shape from figma.py import response:
    ```
    {
      "nodes": {
        "123:456": {
          "document": {
            "id": "123:456", "name": "Login", "type": "FRAME",
            "absoluteBoundingBox": {"x": 0, "y": 0, "width": 375, "height": 812},
            "layoutMode": "VERTICAL", "itemSpacing": 16,
            "paddingLeft": 24, "paddingRight": 24, "paddingTop": 48, "paddingBottom": 24,
            "fills": [{"type": "SOLID", "color": {"r": 1, "g": 1, "b": 1, "a": 1}}],
            "children": [
              {"id": "1:1", "type": "TEXT", "name": "Title", "characters": "Welcome back",
               "style": {"fontFamily": "Inter", "fontSize": 24, "fontWeight": 700, "lineHeightPx": 32},
               "fills": [{"type": "SOLID", "color": {"r": 0.1, "g": 0.1, "b": 0.1, "a": 1}}],
               "absoluteBoundingBox": {"x": 24, "y": 48, "width": 327, "height": 32}},
              {"id": "1:2", "type": "RECTANGLE", "name": "Button",
               "absoluteBoundingBox": {"x": 24, "y": 200, "width": 327, "height": 44},
               "fills": [{"type": "SOLID", "color": {"r": 0.76, "g": 0.48, "b": 0.3, "a": 1}}],
               "cornerRadius": 8},
              {"id": "1:3", "type": "VECTOR", "name": "Icon",
               "absoluteBoundingBox": {"x": 50, "y": 300, "width": 24, "height": 24},
               "fills": [{"type": "SOLID", "color": {"r": 0, "g": 0, "b": 0, "a": 1}}]}
            ]
          }
        }
      },
      "components": {"comp1": {"name": "Button"}},
      "styles": {"style1": {"name": "Heading"}}
    }
    ```
    No mocks needed — transform_design_context is pure.
  </action>
  <verify>pytest backend/tests/test_figma_context.py — all tests pass. No external calls.</verify>
  <done>14 unit tests for design context transformer.</done>
</task>

<task type="auto">
  <name>Task 2: Unit tests for LLM vision and code fences</name>
  <files>backend/tests/test_codegen.py, backend/tests/conftest.py</files>
  <action>
    Create backend/tests/test_codegen.py. Tests for call_llm_vision and strip_code_fences.

    **Test class: TestStripCodeFences**
    1. test_strip_jsx_fences — ```jsx\ncode\n``` → code
    2. test_strip_tsx_fences — ```tsx\ncode\n``` → code
    3. test_strip_javascript_fences — ```javascript\ncode\n``` → code
    4. test_strip_plain_fences — ```\ncode\n``` → code
    5. test_no_fences_returns_original — plain text returned as-is
    6. test_empty_string — empty string handled
    7. test_preserves_inner_backticks — code with backticks inside not broken

    **Test class: TestCallLlmVision** (mocked litellm)
    8. test_vision_call_text_only — messages without image, returns content
    9. test_vision_call_with_image — image_base64 provided, message content becomes list with image_url part
    10. test_vision_call_uses_code_gen_model — verify model kwarg is CODE_GEN_MODEL
    11. test_vision_call_no_response_format — verify response_format NOT passed (Gemini conflict)
    12. test_vision_call_empty_content_raises — empty response content raises or returns error
    13. test_vision_call_logs_on_success — verify log called with correct args (use capsys or mock log)
    14. test_vision_call_logs_on_failure — verify error logged with error_code

    **conftest.py updates:**
    Add mock_llm_vision fixture that mocks litellm.acompletion for vision calls.
    Add mock_prototype_session_db fixture that extends mock_db with prototype_sessions storage:
    - create_prototype_session: stores in memory, returns id
    - update_prototype_session: updates in memory
    - get_prototype_session: retrieves from memory
    Monkeypatch app.db.create_prototype_session, app.db.update_prototype_session, app.db.get_prototype_session.
  </action>
  <verify>pytest backend/tests/test_codegen.py — all tests pass. litellm mocked, no real API calls.</verify>
  <done>14 unit tests for vision LLM and code fence stripping.</done>
</task>

<task type="auto">
  <name>Task 3: Integration tests for code generation API</name>
  <files>backend/tests/test_codegen_api.py</files>
  <action>
    Create backend/tests/test_codegen_api.py. Integration tests for POST /api/code/generate and GET /api/code/session endpoints. Mock: litellm (returns React code), httpx (thumbnail fetch, Figma SVG), DB (prototype_sessions). Follow existing test_api.py patterns.

    **Test class: TestCodeGenerate (POST /api/code/generate)**
    1. test_generate_returns_200_with_session — valid request with session cookie returns {session_id, status: "ready"}
    2. test_generate_no_session_cookie_returns_401 — missing bp_session cookie returns 401
    3. test_generate_no_figma_tokens_returns_401 — session exists but no Figma tokens → 401 with friendly message
    4. test_generate_stores_code_in_db — after successful generation, get_prototype_session returns generated_code
    5. test_generate_transforms_design_context — verify transform_design_context called (not raw dump to LLM)
    6. test_generate_fetches_thumbnail_as_base64 — verify thumbnail URL fetched and base64 encoded for vision
    7. test_generate_retries_on_invalid_jsx — first LLM response is invalid JSX, second is valid → status: "ready"
    8. test_generate_fails_after_retry — both attempts produce invalid JSX → status: "error" with error_code
    9. test_generate_handles_llm_failure — LLM raises exception → status: "error" with error_code
    10. test_generate_handles_thumbnail_fetch_failure — thumbnail URL unreachable → still generates (image_base64=None)
    11. test_generate_error_code_format — error_code matches BP-XXXXXX format

    **Test class: TestCodeSession (GET /api/code/session)**
    12. test_get_session_returns_session — existing session returns PrototypeSession data
    13. test_get_session_no_session_returns_404 — no session for cookie → 404
    14. test_get_session_no_cookie_returns_404_or_401 — missing cookie → appropriate error

    **Test class: TestCodeGenerateLogging**
    15. test_generate_logs_pipeline_start — verify "code generation started" logged
    16. test_generate_logs_pipeline_complete — verify "code generation completed" logged with duration_ms
    17. test_generate_logs_error_with_error_code — on failure, error logged with error_code

    **Mocking approach:**
    - Mock litellm.acompletion to return React component code (valid JSX):
      ```
      export default function App() { return (<div className="p-6 bg-white"><h1 className="text-2xl font-bold">Welcome back</h1></div>); }
      ```
    - Mock httpx for thumbnail fetch (return bytes)
    - Mock httpx for Figma SVG export (return SVG strings)
    - Use mock_db fixture extended with prototype_sessions (from Task 2 conftest updates)
    - Mock app.db.get_figma_tokens to return valid tokens for session
    - Set bp_session cookie on test client requests: `cookies={"bp_session": "test-session-123"}`
  </action>
  <verify>pytest backend/tests/test_codegen_api.py — all tests pass. All external services mocked. No real API calls to Figma, LLM, or Supabase.</verify>
  <done>17 integration tests for code generation API with mocked externals.</done>
</task>

</tasks>

<verification>
- pytest backend/tests/test_figma_context.py — all pass
- pytest backend/tests/test_codegen.py — all pass
- pytest backend/tests/test_codegen_api.py — all pass
- pytest backend/tests/ — full suite passes (existing + new tests, no regressions)
- No real API calls in any test
- Logging verified in test_codegen_api.py
</verification>

<success_criteria>
- ~45 new tests covering transformer, LLM vision, code fences, API endpoints, and logging
- All external services mocked (litellm, httpx, Supabase)
- Test patterns consistent with existing backend/tests/ conventions
- Existing tests unbroken
</success_criteria>

<output>
After completion, create `.planning/phases/02-code-generation/02-06-SUMMARY.md`
</output>
